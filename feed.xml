<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ajg91.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://ajg91.github.io//" rel="alternate" type="text/html" /><updated>2025-08-28T21:19:28-07:00</updated><id>https://ajg91.github.io//feed.xml</id><title type="html">AJG’s Homepage</title><subtitle>A place for my projects.</subtitle><entry><title type="html">Linear regression: Applying Scikit-Learn to projectile motion example</title><link href="https://ajg91.github.io//blog/2025/08/23/linear-regression-with-scikit-learn.html" rel="alternate" type="text/html" title="Linear regression: Applying Scikit-Learn to projectile motion example" /><published>2025-08-23T00:00:00-07:00</published><updated>2025-08-23T00:00:00-07:00</updated><id>https://ajg91.github.io//blog/2025/08/23/linear-regression-with-scikit-learn</id><content type="html" xml:base="https://ajg91.github.io//blog/2025/08/23/linear-regression-with-scikit-learn.html"><![CDATA[<p class="post-date">August 23, 2025</p>

<p>Last time, we went over projectile motion and how to code the equations up in Python. In this post, I will show how to apply linear regression in Python using Scikit-learn’s LinearRegression.</p>

<p>The example we will be working with is a standard two-dimensional kinematics problem. We will have data for only the first two-thirds of the path traveled and we will use linear regression to predict positions past what we have. I will exclusively work with PyTorch tensors and only convert to numpy arrays when needed.</p>

<p>We will be using the class <em>sklearn.linear_model.LinearRegression</em>. The following code applies the linear regression algorithm to our data using Scikit-learn:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_rand_numbers</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">tc</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">+</span> <span class="n">a</span>

<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">t</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">t_rand</span> <span class="o">=</span> <span class="nf">generate_rand_numbers</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="nf">pos_eq</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">v0x</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">t_rand</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">pos_eq</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">v0y</span><span class="p">,</span> <span class="n">ay</span><span class="p">,</span> <span class="n">t_rand</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">t_poly</span> <span class="o">=</span> <span class="n">tc</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">t_rand</span><span class="p">,</span> <span class="n">t_rand</span><span class="o">**</span><span class="mi">2</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t_poly_pred</span> <span class="o">=</span> <span class="n">tc</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">t_pred</span><span class="p">,</span> <span class="n">t_pred</span><span class="o">**</span><span class="mi">2</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">t_rand</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">x_skl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">t_rand</span><span class="p">))</span>
<span class="n">x_skl_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">t_pred</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">t_poly</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_skl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">t_poly</span><span class="p">))</span>
<span class="n">y_skl_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">t_poly_pred</span><span class="p">))</span>
</code></pre></div></div>

<p>The process is as follows:</p>
<ol>
  <li>Generate random times within some interval give by the start and end time.</li>
  <li>Calculate the position values of of random times for both $x$ and $y$.</li>
  <li>Initialize the model.</li>
  <li>Fit the model using the random times and their associated positions.</li>
  <li>Use the model to predict the positions at new, unseen times.</li>
</ol>

<p>One thing to note:<br />
For the $y$ position, we need to reshape the input variables $t_\text{rand}$ and $t_\text{pred}$ as shown by $t_\text{poly}$ and $t_\text{poly_pred}$. The reason is that the equations are not linear, but the class expects to perform a linear regression. The key here is that although the equations are not linear in $t$, they are in fact linear in the parameters that the model is trying to learn as shown below:</p>

\[y \approx w_1 x + w_2 x^2 + w_3 x^3 + b,\]

<p>where the parameters the model is trying to fit are $w_1$, $w_2$, $w_3$, and $b$.</p>

<p>Here are the results of the linear regression:</p>

<p><img src="/assets/images/linear_regression/linear_reg_skl.png" alt="lr_skl" /></p>

<p>The following plot shows how the trajectory would look to a person watching the object being thrown.</p>

<p><img src="/assets/images/linear_regression/linear_reg_skl_y_vs_x.png" alt="lr_skl_y_vs_x" /></p>

<p>The plot on the left shows the linear regression process for the $x$ position and the one on the right for the $y$ position. From the legend, we see that the true values that were used to pick the random times are given in yellow triangles, the true values for the times we are trying to predict are given by the brown line, the training values ($t_\text{rand}$) are given by the black x, the predicted position of the $t_\text{rand}$ values are given by blue dots, and the prediction for the new times are given by red dots. As can be seen, the model does a good job of predicting the positions for the times we do not have data for since the predictions match up with the truth.</p>

<p><img src="/assets/images/linear_regression/linear_reg_abs_err_skl.png" alt="lr_skl_abs_err" /></p>

<p>The absolute errors are shown above. The overall mean square error (MSE) is on the order of $10^{-2}$, which in this case is good enough since we are dealing with meters. The reason as to why the MSE is not smaller is because of the noise we added to the data. In this case, the noise is the most dominant error, which prevents the model from achieving a more accurate prediction.</p>

<p>Next time, I will go over how to perform linear regression using PyTorch. Stay tuned!</p>

<p><a href="/" class="btn">⬅ Back to Home</a></p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[August 23, 2025 Last time, we went over projectile motion and how to code the equations up in Python. In this post, I will show how to apply linear regression in Python using Scikit-learn’s LinearRegression. The example we will be working with is a standard two-dimensional kinematics problem. We will have data for only the first two-thirds of the path traveled and we will use linear regression to predict positions past what we have. I will exclusively work with PyTorch tensors and only convert to numpy arrays when needed. We will be using the class sklearn.linear_model.LinearRegression. The following code applies the linear regression algorithm to our data using Scikit-learn: def generate_rand_numbers(a, b, N): return (b - a) * tc.rand(N) + a a, b = t[0], t[-1] t_rand = generate_rand_numbers(a, b, t.shape[0]).reshape(-1, 1) x = pos_eq(x0, v0x, ax, t_rand, True) y = pos_eq(y0, v0y, ay, t_rand, True) t_poly = tc.concat([t_rand, t_rand**2], dim=-1) t_poly_pred = tc.cat([t_pred, t_pred**2], dim=1) model = LinearRegression() model.fit(t_rand, x) x_skl = np.squeeze(model.predict(t_rand)) x_skl_pred = np.squeeze(model.predict(t_pred)) model = LinearRegression() model.fit(t_poly, y) y_skl = np.squeeze(model.predict(t_poly)) y_skl_pred = np.squeeze(model.predict(t_poly_pred)) The process is as follows: Generate random times within some interval give by the start and end time. Calculate the position values of of random times for both $x$ and $y$. Initialize the model. Fit the model using the random times and their associated positions. Use the model to predict the positions at new, unseen times. One thing to note: For the $y$ position, we need to reshape the input variables $t_\text{rand}$ and $t_\text{pred}$ as shown by $t_\text{poly}$ and $t_\text{poly_pred}$. The reason is that the equations are not linear, but the class expects to perform a linear regression. The key here is that although the equations are not linear in $t$, they are in fact linear in the parameters that the model is trying to learn as shown below: \[y \approx w_1 x + w_2 x^2 + w_3 x^3 + b,\] where the parameters the model is trying to fit are $w_1$, $w_2$, $w_3$, and $b$. Here are the results of the linear regression: The following plot shows how the trajectory would look to a person watching the object being thrown. The plot on the left shows the linear regression process for the $x$ position and the one on the right for the $y$ position. From the legend, we see that the true values that were used to pick the random times are given in yellow triangles, the true values for the times we are trying to predict are given by the brown line, the training values ($t_\text{rand}$) are given by the black x, the predicted position of the $t_\text{rand}$ values are given by blue dots, and the prediction for the new times are given by red dots. As can be seen, the model does a good job of predicting the positions for the times we do not have data for since the predictions match up with the truth. The absolute errors are shown above. The overall mean square error (MSE) is on the order of $10^{-2}$, which in this case is good enough since we are dealing with meters. The reason as to why the MSE is not smaller is because of the noise we added to the data. In this case, the noise is the most dominant error, which prevents the model from achieving a more accurate prediction. Next time, I will go over how to perform linear regression using PyTorch. Stay tuned! ⬅ Back to Home]]></summary></entry><entry><title type="html">Linear regression: A review of projectile motion</title><link href="https://ajg91.github.io//blog/2025/08/16/projectile-motion.html" rel="alternate" type="text/html" title="Linear regression: A review of projectile motion" /><published>2025-08-16T00:00:00-07:00</published><updated>2025-08-16T00:00:00-07:00</updated><id>https://ajg91.github.io//blog/2025/08/16/projectile-motion</id><content type="html" xml:base="https://ajg91.github.io//blog/2025/08/16/projectile-motion.html"><![CDATA[<p class="post-date">August 16, 2025</p>

<p>With all the buzz surrounding AI and machine learning, it is not uncommon to hear the term ‘‘linear regression’’ being thrown around. But what is linear regression? Linear regression is a technique that can be used to model the relationship between a dependent quantity and an independent quantity. For example, the weather is a dependent quantity that depends on the time of day, which is the independent quantity. The method is called linear because it models using a linear relationship. Overall, you are trying to learn the relationship between two variables using a linear fit.</p>

<p>With linear regression, one can then use the independent quantities to predict the its dependent counterpart. For example, if we applied linear regression to a data composed of energies and their corresponding potential energies, we can make predictions on energies that we <em>do not</em> have data on.</p>

<p>Before that, I will do a quick review on projectile motion and show how to code the kinematic equations up in Python. Kinematics is the study of how objects move, with the focus being on quantities such as time, velocity, and position as opposed to forces. Some basics equations that describe kinematics in one-dimension are</p>

\[\begin{aligned}
x &amp;= x_0 + v_0 t + \frac{1}{2} a t^2,\\
v &amp;= v_0 + a t^2.
\end{aligned}\]

<p>Here, $x$ is the final position, $v$ is the final velocity, $x_0$ is the initial position, $v_0$ is the initial velocity, $a$ is the acceleration, and $t$ is the time. If we want to consider motion in two dimensions, we must increase the number of equations. Now, we have</p>

\[\begin{aligned}
x &amp;= x_0 + v_{x0} t,\\
y &amp;= y_0 + v_{y0} t + \frac{1}{2} g t^2,\\ \\[0.1em]
v_{x0} &amp;= v_0 cos(\theta),\\
v_{y0} &amp;= v_0 sin(\theta),\\ \\[0.1em]
v_x &amp;= v_{x0},\\
v_y &amp;= v_{y0} + g t^2,
\end{aligned}\]

<p>where now we have added equations for $x$ and $y$ independently. Here, $g$ is the acceleration due to gravity $g = -9.81 \, m/s^2$. A keen eye would notice that there is an acceleration term in the $y$ equations, but not in the $x$ equations. This is because the only acceleration present is the one due to gravity. More complicated problem can model other accelerations, but for a simple kinematics problem $y$ will only have acceleration due to gravity and $x$ will have no acceleration. Here is a graph of the equation with the accompanying Python code:</p>

<p><img src="/assets/images/linear_regression/proj_motion_plot.png" alt="proj_motion" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pos_eq</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">v0</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p0</span> <span class="o">+</span> <span class="n">v0</span><span class="o">*</span><span class="n">t</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">a</span><span class="o">*</span><span class="n">t</span><span class="o">**</span><span class="mi">2</span>

    <span class="k">if</span> <span class="n">noise</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">+=</span> <span class="n">tc</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>

<span class="n">noise</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">tc</span><span class="p">.</span><span class="nf">asarray</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">tc</span><span class="p">.</span><span class="nf">asarray</span><span class="p">([</span><span class="o">-</span><span class="mf">9.81</span><span class="p">])</span>

<span class="n">x0</span> <span class="o">=</span> <span class="n">tc</span><span class="p">.</span><span class="nf">asarray</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">tc</span><span class="p">.</span><span class="nf">asarray</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">v0</span> <span class="o">=</span> <span class="n">tc</span><span class="p">.</span><span class="nf">asarray</span><span class="p">([</span><span class="mi">40</span><span class="p">])</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">tc</span><span class="p">.</span><span class="nf">asarray</span><span class="p">([</span><span class="mi">45</span><span class="p">])</span>
<span class="n">v0x</span> <span class="o">=</span> <span class="n">v0</span><span class="o">*</span><span class="n">tc</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="n">v0y</span> <span class="o">=</span> <span class="n">v0</span><span class="o">*</span><span class="n">tc</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">tc</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">x_t</span> <span class="o">=</span> <span class="nf">pos_eq</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">v0x</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="nf">pos_eq</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">v0y</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>

<span class="n">t_pred</span> <span class="o">=</span> <span class="n">tc</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x_t_pred</span> <span class="o">=</span> <span class="nf">pos_eq</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">v0x</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">t_pred</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
<span class="n">y_t_pred</span> <span class="o">=</span> <span class="nf">pos_eq</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">v0y</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">t_pred</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
</code></pre></div></div>

<p>As we can see from the code I will be modeling an object being thrown at a $45$ degree angle with a velocity of $v_0 = 40$ m/s. The initial start point will be $x_0 = y_0 = 0$ m. The object will travel for $t = 5$ s. We also define times where the model will be trying to predict the position, $t_\text{pred}$. In order to draw comparisons between the model and the truth, we calculate what the true positions should be. We also choose to add a small amount of random Gaussian noise to the positions.</p>

<p>Next time, I will go over using Scikit-learn to apply linear regression to the projectile motion problem we just went over. Stay tuned!</p>

<p><a href="/" class="btn">⬅ Back to Home</a></p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[August 16, 2025 With all the buzz surrounding AI and machine learning, it is not uncommon to hear the term ‘‘linear regression’’ being thrown around. But what is linear regression? Linear regression is a technique that can be used to model the relationship between a dependent quantity and an independent quantity. For example, the weather is a dependent quantity that depends on the time of day, which is the independent quantity. The method is called linear because it models using a linear relationship. Overall, you are trying to learn the relationship between two variables using a linear fit. With linear regression, one can then use the independent quantities to predict the its dependent counterpart. For example, if we applied linear regression to a data composed of energies and their corresponding potential energies, we can make predictions on energies that we do not have data on. Before that, I will do a quick review on projectile motion and show how to code the kinematic equations up in Python. Kinematics is the study of how objects move, with the focus being on quantities such as time, velocity, and position as opposed to forces. Some basics equations that describe kinematics in one-dimension are \[\begin{aligned} x &amp;= x_0 + v_0 t + \frac{1}{2} a t^2,\\ v &amp;= v_0 + a t^2. \end{aligned}\] Here, $x$ is the final position, $v$ is the final velocity, $x_0$ is the initial position, $v_0$ is the initial velocity, $a$ is the acceleration, and $t$ is the time. If we want to consider motion in two dimensions, we must increase the number of equations. Now, we have \[\begin{aligned} x &amp;= x_0 + v_{x0} t,\\ y &amp;= y_0 + v_{y0} t + \frac{1}{2} g t^2,\\ \\[0.1em] v_{x0} &amp;= v_0 cos(\theta),\\ v_{y0} &amp;= v_0 sin(\theta),\\ \\[0.1em] v_x &amp;= v_{x0},\\ v_y &amp;= v_{y0} + g t^2, \end{aligned}\] where now we have added equations for $x$ and $y$ independently. Here, $g$ is the acceleration due to gravity $g = -9.81 \, m/s^2$. A keen eye would notice that there is an acceleration term in the $y$ equations, but not in the $x$ equations. This is because the only acceleration present is the one due to gravity. More complicated problem can model other accelerations, but for a simple kinematics problem $y$ will only have acceleration due to gravity and $x$ will have no acceleration. Here is a graph of the equation with the accompanying Python code: def pos_eq(p0, v0, a, t, noise=False): p = p0 + v0*t + 0.5*a*t**2 if noise: p += tc.rand(t.shape) return p noise = True ax = tc.asarray([0]) g = tc.asarray([-9.81]) x0 = tc.asarray([0]) y0 = tc.asarray([0]) v0 = tc.asarray([40]) theta = tc.asarray([45]) v0x = v0*tc.cos(theta) v0y = v0*tc.sin(theta) t = tc.linspace(0, 5, 20) x_t = pos_eq(x0, v0x, ax, t, noise) y_t = pos_eq(y0, v0y, g, t, noise) t_pred = tc.linspace(5.5, 8, 8).reshape(-1, 1) x_t_pred = pos_eq(x0, v0x, ax, t_pred, noise) y_t_pred = pos_eq(y0, v0y, g, t_pred, noise) As we can see from the code I will be modeling an object being thrown at a $45$ degree angle with a velocity of $v_0 = 40$ m/s. The initial start point will be $x_0 = y_0 = 0$ m. The object will travel for $t = 5$ s. We also define times where the model will be trying to predict the position, $t_\text{pred}$. In order to draw comparisons between the model and the truth, we calculate what the true positions should be. We also choose to add a small amount of random Gaussian noise to the positions. Next time, I will go over using Scikit-learn to apply linear regression to the projectile motion problem we just went over. Stay tuned! ⬅ Back to Home]]></summary></entry></feed>